{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T01:57:22.706441Z",
     "iopub.status.busy": "2025-01-24T01:57:22.706167Z",
     "iopub.status.idle": "2025-01-24T01:57:40.574574Z",
     "shell.execute_reply": "2025-01-24T01:57:40.573390Z",
     "shell.execute_reply.started": "2025-01-24T01:57:22.706416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T13:02:28.852742Z",
     "iopub.status.busy": "2025-01-19T13:02:28.852205Z",
     "iopub.status.idle": "2025-01-19T13:02:29.214873Z",
     "shell.execute_reply": "2025-01-19T13:02:29.213629Z",
     "shell.execute_reply.started": "2025-01-19T13:02:28.852717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# training_set = tf.keras.utils.image_dataset_from_directory(\n",
    "#     '/content/drive/MyDrive/Fruit_Vegetable_Recognition/train',\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"categorical\",\n",
    "#     class_names=None,\n",
    "#     color_mode=\"rgb\",\n",
    "#     batch_size=32,\n",
    "#     image_size=(64, 64),\n",
    "#     shuffle=True,\n",
    "#     seed=None,\n",
    "#     validation_split=None,\n",
    "#     subset=None,\n",
    "#     interpolation=\"bilinear\",\n",
    "#     follow_links=False,\n",
    "#     crop_to_aspect_ratio=False\n",
    "# )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/blob/v3.8.0/keras/src/utils/image_dataset_utils.py#L12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T06:57:19.193660Z",
     "iopub.status.busy": "2025-01-20T06:57:19.193393Z",
     "iopub.status.idle": "2025-01-20T06:57:22.008078Z",
     "shell.execute_reply": "2025-01-20T06:57:22.007176Z",
     "shell.execute_reply.started": "2025-01-20T06:57:19.193631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def get_folder_names(base_path):\n",
    "#     folder_names = []\n",
    "#     for root, dirs, files in os.walk(base_path):  # os.walk iterates through all subdirectories\n",
    "#         for dir_name in dirs:\n",
    "#             folder_names.append(os.path.join(root, dir_name))  # Full path to the folder\n",
    "#     return folder_names\n",
    "\n",
    "# # Example usage\n",
    "# base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "# folders = get_folder_names(base_path)\n",
    "# print(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sapumicha', 'chhoila', 'Voj set full', 'Thoo', 'yomari', 'Goramari', 'kauli chana', 'Saag', 'chicken', 'Bhuti', 'Paun Kwa', 'Dhau', 'Channa', 'Phokso Fry', 'Lain tarkari', 'Fish', 'labha palu musya', 'Chatamari', 'Aila', 'golveda achar', 'Bhutan', 'Chhoila Baji', 'Mutumari', 'Kachilaa', 'Bara', 'Baji', 'yomari']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Full paths list\n",
    "folder_paths = ['/kaggle/input/newari-food-dataset/newa_food_dataset/sapumicha', '/kaggle/input/newari-food-dataset/newa_food_dataset/chhoila', '/kaggle/input/newari-food-dataset/newa_food_dataset/Voj set full', '/kaggle/input/newari-food-dataset/newa_food_dataset/Thoo', '/kaggle/input/newari-food-dataset/newa_food_dataset/yomari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Goramari', '/kaggle/input/newari-food-dataset/newa_food_dataset/kauli chana', '/kaggle/input/newari-food-dataset/newa_food_dataset/Saag', '/kaggle/input/newari-food-dataset/newa_food_dataset/chicken', '/kaggle/input/newari-food-dataset/newa_food_dataset/Bhuti', '/kaggle/input/newari-food-dataset/newa_food_dataset/Paun Kwa', '/kaggle/input/newari-food-dataset/newa_food_dataset/Dhau', '/kaggle/input/newari-food-dataset/newa_food_dataset/Channa', '/kaggle/input/newari-food-dataset/newa_food_dataset/Phokso Fry', '/kaggle/input/newari-food-dataset/newa_food_dataset/Lain tarkari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Fish', '/kaggle/input/newari-food-dataset/newa_food_dataset/labha palu musya', '/kaggle/input/newari-food-dataset/newa_food_dataset/Chatamari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Aila', '/kaggle/input/newari-food-dataset/newa_food_dataset/golveda achar', '/kaggle/input/newari-food-dataset/newa_food_dataset/Bhutan', '/kaggle/input/newari-food-dataset/newa_food_dataset/Chhoila Baji', '/kaggle/input/newari-food-dataset/newa_food_dataset/Mutumari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Kachilaa', '/kaggle/input/newari-food-dataset/newa_food_dataset/Bara', '/kaggle/input/newari-food-dataset/newa_food_dataset/Baji', '/kaggle/input/newari-food-dataset/newa_food_dataset/yomari/yomari']\n",
    "# Extract only folder names\n",
    "folder_names = [os.path.basename(path) for path in folder_paths]\n",
    "\n",
    "# Print the result\n",
    "print(folder_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T13:16:01.352737Z",
     "iopub.status.busy": "2025-01-19T13:16:01.352403Z",
     "iopub.status.idle": "2025-01-19T13:16:12.741343Z",
     "shell.execute_reply": "2025-01-19T13:16:12.740511Z",
     "shell.execute_reply.started": "2025-01-19T13:16:01.352709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install google-images-download\n",
    "# !pip install pillow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code for Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T08:39:16.153078Z",
     "iopub.status.busy": "2025-01-22T08:39:16.152738Z",
     "iopub.status.idle": "2025-01-22T09:10:41.358447Z",
     "shell.execute_reply": "2025-01-22T09:10:41.357337Z",
     "shell.execute_reply.started": "2025-01-22T08:39:16.153045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import imgaug.augmenters as iaa\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def augment_images_in_folder(input_folder, output_folder, target_count=600):\n",
    "#     \"\"\"\n",
    "#     Augments images in the input folder to ensure at least `target_count` images exist in the output folder.\n",
    "#     \"\"\"\n",
    "#     # Create the output folder if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "#     # List of all image files in the input folder\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "#     image_count = len(image_files)\n",
    "    \n",
    "#     # If the folder already has enough images, skip augmentation\n",
    "#     if image_count >= target_count:\n",
    "#         print(f\"{input_folder} already has {image_count} images, skipping augmentation.\")\n",
    "#         return\n",
    "\n",
    "#     # Load all images\n",
    "#     images = [cv2.imread(os.path.join(input_folder, f)) for f in image_files]\n",
    "#     images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images if img is not None]\n",
    "\n",
    "#     # Define augmentation pipeline\n",
    "#     augmenter = iaa.Sequential([\n",
    "#         iaa.Fliplr(0.5),  # Horizontal flip\n",
    "#         iaa.Affine(scale=(0.8, 1.2), rotate=(-25, 25)),  # Scaling and rotation\n",
    "#         iaa.AdditiveGaussianNoise(scale=(10, 20)),  # Adding noise\n",
    "#         iaa.Multiply((0.8, 1.2)),  # Brightness adjustment\n",
    "#         iaa.Crop(percent=(0, 0.1))  # Random cropping\n",
    "#     ])\n",
    "\n",
    "#     # Augment images until we reach the target count\n",
    "#     augmented_count = 0\n",
    "#     for _ in tqdm(range(target_count - image_count), desc=f\"Augmenting {input_folder}\"):\n",
    "#         # Choose a random image to augment\n",
    "#         random_image = images[np.random.randint(0, len(images))]\n",
    "#         augmented_image = augmenter(image=random_image)\n",
    "\n",
    "#         # Save the augmented image\n",
    "#         output_path = os.path.join(output_folder, f\"aug_{augmented_count + image_count}.jpg\")\n",
    "#         cv2.imwrite(output_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "#         augmented_count += 1\n",
    "\n",
    "#     print(f\"Augmented {augmented_count} images in {input_folder}.\")\n",
    "\n",
    "\n",
    "# def augment_dataset(base_path, output_base_path, target_count=250):\n",
    "#     \"\"\"\n",
    "#     Iterates through each folder in the base path and augments images to reach the target count.\n",
    "#     \"\"\"\n",
    "#     folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "#     for folder in folders:\n",
    "#         input_folder = os.path.join(base_path, folder)\n",
    "#         output_folder = os.path.join(output_base_path, folder)\n",
    "#         augment_images_in_folder(input_folder, output_folder, target_count)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "# output_base_path = \"/kaggle/working/augmented_dataset\"  # Output directory for augmented images\n",
    "# target_count = 600  # Target number of images per folder\n",
    "\n",
    "# augment_dataset(base_path, output_base_path, target_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import imgaug.augmenters as iaa\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def augment_images_in_folder(input_folder, output_folder, target_count=600):\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "#     image_count = len(image_files)\n",
    "#     if image_count >= target_count:\n",
    "#         print(f\"{input_folder} already has {image_count} images, skipping augmentation.\")\n",
    "#         return\n",
    "\n",
    "#     images = [cv2.imread(os.path.join(input_folder, f)) for f in image_files]\n",
    "#     images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images if img is not None]\n",
    "\n",
    "#     augmenter = iaa.Sequential([\n",
    "#         iaa.Fliplr(0.5),\n",
    "#         iaa.Affine(scale=(0.8, 1.2), rotate=(-25, 25)),\n",
    "#         iaa.AdditiveGaussianNoise(scale=(10, 20)),\n",
    "#         iaa.Multiply((0.8, 1.2)),\n",
    "#         iaa.Crop(percent=(0, 0.1))\n",
    "#     ])\n",
    "\n",
    "#     augmented_count = 0\n",
    "#     for _ in tqdm(range(target_count - image_count), desc=f\"Augmenting {input_folder}\"):\n",
    "#         random_image = images[np.random.randint(0, len(images))]\n",
    "#         augmented_image = augmenter(image=random_image)\n",
    "#         output_path = os.path.join(output_folder, f\"aug_{augmented_count + image_count}.jpg\")\n",
    "#         cv2.imwrite(output_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "#         augmented_count += 1\n",
    "\n",
    "#     print(f\"Augmented {augmented_count} images in {input_folder}.\")\n",
    "\n",
    "# def augment_dataset(base_path, output_base_path, target_count=600, folder_indices=None):\n",
    "#     if folder_indices is None:\n",
    "#         folder_indices = []\n",
    "#     folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "#     selected_folders = [folders[i] for i in folder_indices]\n",
    "#     for folder in selected_folders:\n",
    "#         input_folder = os.path.join(base_path, folder)\n",
    "#         output_folder = os.path.join(output_base_path, folder)\n",
    "#         augment_images_in_folder(input_folder, output_folder, target_count)\n",
    "\n",
    "# # Base paths\n",
    "# base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "# output_base_path = \"/kaggle/working/augmented_dataset\"\n",
    "# target_count = 600\n",
    "\n",
    "# # First 8 folders (indices 0 to 7)\n",
    "# augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:33:25.117395Z",
     "iopub.status.busy": "2025-01-22T15:33:25.117132Z",
     "iopub.status.idle": "2025-01-22T15:53:31.345711Z",
     "shell.execute_reply": "2025-01-22T15:53:31.344728Z",
     "shell.execute_reply.started": "2025-01-22T15:33:25.117361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # best ever explaination, differences, roles in the real world and road map for the datascience , analyst and engineer to give overview to all who donot know anything about theseimport os\n",
    "# import cv2\n",
    "# import imgaug.augmenters as iaa\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def augment_images_in_folder(input_folder, output_folder, target_count=600):\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "#     image_count = len(image_files)\n",
    "#     if image_count >= target_count:\n",
    "#         print(f\"{input_folder} already has {image_count} images, skipping augmentation.\")\n",
    "#         return\n",
    "\n",
    "#     images = [cv2.imread(os.path.join(input_folder, f)) for f in image_files]\n",
    "#     images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images if img is not None]\n",
    "\n",
    "#     augmenter = iaa.Sequential([\n",
    "#         iaa.Fliplr(0.5),\n",
    "#         iaa.Affine(scale=(0.8, 1.2), rotate=(-25, 25)),\n",
    "#         iaa.AdditiveGaussianNoise(scale=(10, 20)),\n",
    "#         iaa.Multiply((0.8, 1.2)),\n",
    "#         iaa.Crop(percent=(0, 0.1))\n",
    "#     ])\n",
    "\n",
    "#     augmented_count = 0\n",
    "#     for _ in tqdm(range(target_count - image_count), desc=f\"Augmenting {input_folder}\"):\n",
    "#         random_image = images[np.random.randint(0, len(images))]\n",
    "#         augmented_image = augmenter(image=random_image)\n",
    "#         output_path = os.path.join(output_folder, f\"aug_{augmented_count + image_count}.jpg\")\n",
    "#         cv2.imwrite(output_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "#         augmented_count += 1\n",
    "\n",
    "#     print(f\"Augmented {augmented_count} images in {input_folder}.\")\n",
    "\n",
    "# def augment_dataset(base_path, output_base_path, target_count=600, folder_indices=None):\n",
    "#     if folder_indices is None:\n",
    "#         folder_indices = []\n",
    "#     folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "#     selected_folders = [folders[i] for i in folder_indices]\n",
    "#     for folder in selected_folders:\n",
    "#         input_folder = os.path.join(base_path, folder)\n",
    "#         output_folder = os.path.join(output_base_path, folder)\n",
    "#         augment_images_in_folder(input_folder, output_folder, target_count)\n",
    "\n",
    "# # Base paths\n",
    "# base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "# output_base_path = \"/kaggle/working/augmented_dataset_part3\"\n",
    "# target_count = 600\n",
    "\n",
    "# # First 8 folders (indices 0 to 7)\n",
    "# # augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8)))\n",
    "# best ever explaination, differences, roles in the real world and road map for the datascience , analyst and engineer to give overview to all who donot know anything about these\n",
    "# # Second batch: Remaining 8 folders (indices 8 to 15)\n",
    "# augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(12, 16)))\n",
    "\n",
    "# # augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8, 12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-22T14:51:53.637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Second batch: Remaining 8 folders (indices 8 to 15)\n",
    "# augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8, 16)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T15:56:25.353305Z",
     "iopub.status.busy": "2025-01-22T15:56:25.353005Z",
     "iopub.status.idle": "2025-01-22T16:01:06.063452Z",
     "shell.execute_reply": "2025-01-22T16:01:06.062203Z",
     "shell.execute_reply.started": "2025-01-22T15:56:25.353280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped folder created at: /kaggle/working/augmented_dataset_600_part3.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the folder to zip and the output zip file path\n",
    "folder_to_zip = \"/kaggle/working/augmented_dataset_part3\"\n",
    "output_zip_file = \"/kaggle/working/augmented_dataset_600_part3.zip\"\n",
    "\n",
    "# Create a zip archive of the folder2\n",
    "shutil.make_archive(base_name=output_zip_file.replace('.zip', ''), format='zip', root_dir=folder_to_zip)\n",
    "\n",
    "print(f\"Zipped folder created at: {output_zip_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T01:58:41.015995Z",
     "iopub.status.busy": "2025-01-22T01:58:41.015722Z",
     "iopub.status.idle": "2025-01-22T01:58:41.021105Z",
     "shell.execute_reply": "2025-01-22T01:58:41.020135Z",
     "shell.execute_reply.started": "2025-01-22T01:58:41.015974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied to new_dataset_training_splitted_0.7\n",
      "Class 'aloo': 0 train, 0 test\n",
      "Class 'Baji': 0 train, 0 test\n",
      "Class 'Bara': 0 train, 0 test\n",
      "Class 'Bhutan': 0 train, 0 test\n",
      "Class 'Bhuti': 0 train, 0 test\n",
      "Class 'Buffalo Masu(Dakulaa)': 0 train, 0 test\n",
      "Class 'Channa': 0 train, 0 test\n",
      "Class 'Dhau': 0 train, 0 test\n",
      "Class 'egg': 0 train, 0 test\n",
      "Class 'Kachilaa': 0 train, 0 test\n",
      "Class 'Lain tarkari': 0 train, 0 test\n",
      "Class 'Lainachar': 0 train, 0 test\n",
      "Class 'Saag': 0 train, 0 test\n",
      "Class 'sapumicha': 0 train, 0 test\n",
      "Class 'test': 0 train, 0 test\n",
      "Class 'Thoo': 0 train, 0 test\n",
      "Class 'train': 0 train, 0 test\n",
      "Class 'val': 0 train, 0 test\n",
      "Class 'yomari': 0 train, 0 test\n",
      "Zipped dataset saved to new_dataset_training_splitted_0.8_testing_0.2.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Define paths\n",
    "input_base_path = 'augmented_dataset_600'\n",
    "working_base_path = 'new_dataset_training_splitted_0.7'\n",
    "\n",
    "# Copy dataset to a writable directory\n",
    "def copy_dataset(input_path, working_path):\n",
    "    if not os.path.exists(working_path):\n",
    "        shutil.copytree(input_path, working_path)\n",
    "    print(f\"Dataset copied to {working_path}\")\n",
    "\n",
    "# Define proportions\n",
    "# train_ratio = 0.7\n",
    "train_ratio = 0.8\n",
    "# test_ratio = 0.15\n",
    "test_ratio = 0.2\n",
    "# val_ratio = 0.15\n",
    "\n",
    "# Ensure the ratios add up to 1\n",
    "# assert train_ratio + test_ratio + val_ratio == 1, \"Ratios must sum to 1.\"\n",
    "assert train_ratio + test_ratio  == 1, \"Ratios must sum to 1.\"\n",
    "\n",
    "# Function to split dataset\n",
    "def split_dataset(base_path):\n",
    "    for class_name in os.listdir(base_path):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        # Get all images in the current class folder\n",
    "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "\n",
    "        # Shuffle the images\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Compute split indices\n",
    "        total_images = len(images)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        test_count = int(total_images * test_ratio)\n",
    "\n",
    "        # Split images into train, test, and validation sets\n",
    "        train_images = images[:train_count]\n",
    "        test_images = images[train_count:train_count + test_count]\n",
    "        # val_images = images[train_count + test_count:]\n",
    "\n",
    "        # Create train, test, and validation directories\n",
    "        train_dir = os.path.join(class_path, 'train')\n",
    "        test_dir = os.path.join(class_path, 'test')\n",
    "        # val_dir = os.path.join(class_path, 'val')\n",
    "\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "        # os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "        # Move images to respective folders\n",
    "        for img in train_images:\n",
    "            shutil.move(os.path.join(class_path, img), os.path.join(train_dir, img))\n",
    "\n",
    "        for img in test_images:\n",
    "            shutil.move(os.path.join(class_path, img), os.path.join(test_dir, img))\n",
    "\n",
    "        # for img in val_images:\n",
    "            # shutil.move(os.path.join(class_path, img), os.path.join(val_dir, img))\n",
    "\n",
    "        # print(f\"Class '{class_name}': {len(train_images)} train, {len(test_images)} test, {len(val_images)} val\")\n",
    "        print(f\"Class '{class_name}': {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "# Function to zip the dataset\n",
    "def zip_dataset(base_path, output_zip):\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, base_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "    print(f\"Zipped dataset saved to {output_zip}\")\n",
    "\n",
    "# Copy the dataset to a writable directory\n",
    "copy_dataset(input_base_path, working_base_path)\n",
    "\n",
    "# Run the split function\n",
    "split_dataset(working_base_path)\n",
    "\n",
    "# Create a zip file of the split dataset\n",
    "output_zip = 'new_dataset_training_splitted_0.8_testing_0.2.zip'\n",
    "zip_dataset(working_base_path, output_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'aloo': 0 train, 0 test, 0 val\n",
      "Class 'Baji': 0 train, 0 test, 0 val\n",
      "Class 'Bara': 0 train, 0 test, 0 val\n",
      "Class 'Bhutan': 0 train, 0 test, 0 val\n",
      "Class 'Bhuti': 0 train, 0 test, 0 val\n",
      "Class 'Buffalo Masu(Dakulaa)': 0 train, 0 test, 0 val\n",
      "Class 'Channa': 0 train, 0 test, 0 val\n",
      "Class 'Dhau': 0 train, 0 test, 0 val\n",
      "Class 'egg': 0 train, 0 test, 0 val\n",
      "Class 'Kachilaa': 0 train, 0 test, 0 val\n",
      "Class 'Lain tarkari': 0 train, 0 test, 0 val\n",
      "Class 'Lainachar': 0 train, 0 test, 0 val\n",
      "Class 'Saag': 0 train, 0 test, 0 val\n",
      "Class 'sapumicha': 0 train, 0 test, 0 val\n",
      "Class 'test': 0 train, 0 test, 0 val\n",
      "Class 'Thoo': 0 train, 0 test, 0 val\n",
      "Class 'train': 0 train, 0 test, 0 val\n",
      "Class 'val': 0 train, 0 test, 0 val\n",
      "Class 'yomari': 0 train, 0 test, 0 val\n",
      "Dataset copied to new_dataset_training_splitted_0.7\n",
      "Zipped dataset saved to new_dataset_training_splitted_0.7.zip\n"
     ]
    }
   ],
   "source": [
    "# Ensure the ratios add up to 1\n",
    "# assert train_ratio + test_ratio + val_ratio == 1, \"Ratios must sum to 1.\"\n",
    "assert train_ratio + test_ratio , \"Ratios must sum to 1.\"\n",
    "\n",
    "# Function to split dataset\n",
    "def split_dataset(base_path):\n",
    "    # Create train, test, and validation directories in the root folder\n",
    "    train_dir = os.path.join(base_path, 'train')\n",
    "    \n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Define paths\n",
    "base_path = 'augmented_dataset_600'\n",
    "working_base_path = 'new_dataset_training_splitted_0.7'\n",
    "\n",
    "# Copy dataset to a writable directory\n",
    "def copy_dataset(input_path, working_path):\n",
    "    if not os.path.exists(working_path):\n",
    "        shutil.copytree(input_path, working_path)\n",
    "    print(f\"Dataset copied to {working_path}\")\n",
    "\n",
    "# Define proportions\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "val_ratio = 0.15\n",
    "\n",
    "train_dir = os.path.join(base_path, 'train')\n",
    "test_dir = os.path.join(base_path, 'test')\n",
    "val_dir = os.path.join(base_path, 'val')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "for class_name in os.listdir(base_path):\n",
    "    class_path = os.path.join(base_path, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    # Get all images in the current class folder\n",
    "    images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "\n",
    "    # Shuffle the images\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Compute split indices\n",
    "    total_images = len(images)\n",
    "    train_count = int(total_images * train_ratio)\n",
    "    test_count = int(total_images * test_ratio)\n",
    "\n",
    "    # Split images into train, test, and validation sets\n",
    "    train_images = images[:train_count]\n",
    "    test_images = images[train_count:train_count + test_count]\n",
    "    val_images = images[train_count + test_count:]\n",
    "\n",
    "    # Create class-specific folders inside train, test, and validation directories\n",
    "    class_train_dir = os.path.join(train_dir, class_name)\n",
    "    class_test_dir = os.path.join(test_dir, class_name)\n",
    "    class_val_dir = os.path.join(val_dir, class_name)\n",
    "\n",
    "    os.makedirs(class_train_dir, exist_ok=True)\n",
    "    os.makedirs(class_test_dir, exist_ok=True)\n",
    "    os.makedirs(class_val_dir, exist_ok=True)\n",
    "\n",
    "    # Move images to respective folders\n",
    "    for img in train_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(class_train_dir, img))\n",
    "\n",
    "    for img in test_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(class_test_dir, img))\n",
    "\n",
    "    for img in val_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(class_val_dir, img))\n",
    "\n",
    "    # Remove the original class directory if empty\n",
    "    if not os.listdir(class_path):\n",
    "        os.rmdir(class_path)\n",
    "\n",
    "    print(f\"Class '{class_name}': {len(train_images)} train, {len(test_images)} test, {len(val_images)} val\")\n",
    "\n",
    "# Function to zip the dataset\n",
    "def zip_dataset(base_path, output_zip):\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, base_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "    print(f\"Zipped dataset saved to {output_zip}\")\n",
    "\n",
    "# Copy the dataset to a writable directory\n",
    "copy_dataset(input_base_path, working_base_path)\n",
    "\n",
    "# Run the split function\n",
    "split_dataset(working_base_path)\n",
    "\n",
    "# Create a zip file of the split dataset\n",
    "output_zip = 'new_dataset_training_splitted_0.7.zip'\n",
    "zip_dataset(working_base_path, output_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'test': 0 train, 0 test\n",
      "Class 'train': 0 train, 0 test\n",
      "Dataset copied to new_dataset_training_splitted_0.8\n",
      "Zipped dataset saved to new_dataset_training_splitted_0.8and0.2_train_test.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# Define paths\n",
    "base_path = 'augmented_dataset_600'\n",
    "working_base_path = 'new_dataset_training_splitted_0.8'\n",
    "\n",
    "# Function to copy dataset to a writable directory\n",
    "def copy_dataset(input_path, working_path):\n",
    "    if not os.path.exists(working_path):\n",
    "        shutil.copytree(input_path, working_path)\n",
    "    print(f\"Dataset copied to {working_path}\")\n",
    "\n",
    "# Define proportions\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Define directories for split dataset\n",
    "train_dir = os.path.join(working_base_path, 'train')\n",
    "test_dir = os.path.join(working_base_path, 'test')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over all classes (folders inside base_path)\n",
    "for class_name in os.listdir(base_path):\n",
    "    class_path = os.path.join(base_path, class_name)\n",
    "    \n",
    "    if not os.path.isdir(class_path):  # Skip if it's not a folder\n",
    "        continue\n",
    "\n",
    "    # Get all images in the class folder\n",
    "    images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "\n",
    "    # Shuffle images randomly\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Compute split indices\n",
    "    total_images = len(images)\n",
    "    train_count = int(total_images * train_ratio)\n",
    "    test_count = total_images - train_count  # Remaining for test\n",
    "\n",
    "    # Split images into train and test sets\n",
    "    train_images = images[:train_count]\n",
    "    test_images = images[train_count:]\n",
    "\n",
    "    # Create class-specific folders inside train and test directories\n",
    "    class_train_dir = os.path.join(train_dir, class_name)\n",
    "    class_test_dir = os.path.join(test_dir, class_name)\n",
    "\n",
    "    os.makedirs(class_train_dir, exist_ok=True)\n",
    "    os.makedirs(class_test_dir, exist_ok=True)\n",
    "\n",
    "    # Move images to respective directories\n",
    "    for img in train_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(class_train_dir, img))\n",
    "\n",
    "    for img in test_images:\n",
    "        shutil.move(os.path.join(class_path, img), os.path.join(class_test_dir, img))\n",
    "\n",
    "    # Remove original class directory if empty\n",
    "    if not os.listdir(class_path):\n",
    "        os.rmdir(class_path)\n",
    "\n",
    "    print(f\"Class '{class_name}': {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "# Function to zip the dataset\n",
    "def zip_dataset(base_path, output_zip):\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, base_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "    print(f\"Zipped dataset saved to {output_zip}\")\n",
    "\n",
    "# Copy dataset to working directory\n",
    "copy_dataset(base_path, working_base_path)\n",
    "\n",
    "# Zip the dataset\n",
    "output_zip = 'new_dataset_training_splitted_0.8and0.2_train_test.zip'\n",
    "zip_dataset(working_base_path, output_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset copied to new_dataset_training_splitted_0.8\n",
      "Class 'test': 0 train, 0 test\n",
      "Class 'train': 0 train, 0 test\n",
      "Zipped dataset saved to new_dataset_training_splitted_0.8.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def split_dataset(base_path, train_ratio=0.8, test_ratio=0.2):\n",
    "    assert train_ratio + test_ratio == 1, \"Ratios must sum to 1.\"\n",
    "    \n",
    "    train_dir = os.path.join(base_path, 'train')\n",
    "    test_dir = os.path.join(base_path, 'test')\n",
    "    \n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    for class_name in os.listdir(base_path):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        total_images = len(images)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        \n",
    "        train_images = images[:train_count]\n",
    "        test_images = images[train_count:]\n",
    "        \n",
    "        class_train_dir = os.path.join(train_dir, class_name)\n",
    "        class_test_dir = os.path.join(test_dir, class_name)\n",
    "        \n",
    "        os.makedirs(class_train_dir, exist_ok=True)\n",
    "        os.makedirs(class_test_dir, exist_ok=True)\n",
    "        \n",
    "        for img in train_images:\n",
    "            shutil.move(os.path.join(class_path, img), os.path.join(class_train_dir, img))\n",
    "        \n",
    "        for img in test_images:\n",
    "            shutil.move(os.path.join(class_path, img), os.path.join(class_test_dir, img))\n",
    "        \n",
    "        if not os.listdir(class_path):\n",
    "            os.rmdir(class_path)\n",
    "        \n",
    "        print(f\"Class '{class_name}': {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "def zip_dataset(base_path, output_zip):\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, base_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "    print(f\"Zipped dataset saved to {output_zip}\")\n",
    "\n",
    "# Define paths\n",
    "base_path = 'augmented_dataset_600'  # Change this to your dataset path\n",
    "working_base_path = 'new_dataset_training_splitted_0.8'\n",
    "\n",
    "# Copy dataset to a writable directory\n",
    "def copy_dataset(input_path, working_path):\n",
    "    if not os.path.exists(working_path):\n",
    "        shutil.copytree(input_path, working_path)\n",
    "    print(f\"Dataset copied to {working_path}\")\n",
    "\n",
    "copy_dataset(base_path, working_base_path)\n",
    "split_dataset(working_base_path)\n",
    "\n",
    "# Create a zip file of the split dataset\n",
    "output_zip = 'new_dataset_training_splitted_0.8.zip'\n",
    "zip_dataset(working_base_path, output_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6512522,
     "sourceId": 10522682,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6517542,
     "sourceId": 10532705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6519962,
     "sourceId": 10537376,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
