{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdd7d0d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003844,
     "end_time": "2025-01-22T17:09:23.355572",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.351728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c391afd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.362691Z",
     "iopub.status.busy": "2025-01-22T17:09:23.362409Z",
     "iopub.status.idle": "2025-01-22T17:09:23.366088Z",
     "shell.execute_reply": "2025-01-22T17:09:23.365500Z"
    },
    "papermill": {
     "duration": 0.00835,
     "end_time": "2025-01-22T17:09:23.367161",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.358811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1682ad",
   "metadata": {
    "papermill": {
     "duration": 0.002902,
     "end_time": "2025-01-22T17:09:23.373181",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.370279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d1a968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.379950Z",
     "iopub.status.busy": "2025-01-22T17:09:23.379754Z",
     "iopub.status.idle": "2025-01-22T17:09:23.382471Z",
     "shell.execute_reply": "2025-01-22T17:09:23.381865Z"
    },
    "papermill": {
     "duration": 0.00746,
     "end_time": "2025-01-22T17:09:23.383700",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.376240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_set = tf.keras.utils.image_dataset_from_directory(\n",
    "#     '/content/drive/MyDrive/Fruit_Vegetable_Recognition/train',\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"categorical\",\n",
    "#     class_names=None,\n",
    "#     color_mode=\"rgb\",\n",
    "#     batch_size=32,\n",
    "#     image_size=(64, 64),\n",
    "#     shuffle=True,\n",
    "#     seed=None,\n",
    "#     validation_split=None,\n",
    "#     subset=None,\n",
    "#     interpolation=\"bilinear\",\n",
    "#     follow_links=False,\n",
    "#     crop_to_aspect_ratio=False\n",
    "# )\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ecb263",
   "metadata": {
    "papermill": {
     "duration": 0.002814,
     "end_time": "2025-01-22T17:09:23.389618",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.386804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "https://github.com/keras-team/keras/blob/v3.8.0/keras/src/utils/image_dataset_utils.py#L12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99624acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.396547Z",
     "iopub.status.busy": "2025-01-22T17:09:23.396344Z",
     "iopub.status.idle": "2025-01-22T17:09:23.399007Z",
     "shell.execute_reply": "2025-01-22T17:09:23.398377Z"
    },
    "papermill": {
     "duration": 0.007572,
     "end_time": "2025-01-22T17:09:23.400170",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.392598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def get_folder_names(base_path):\n",
    "#     folder_names = []\n",
    "#     for root, dirs, files in os.walk(base_path):  # os.walk iterates through all subdirectories\n",
    "#         for dir_name in dirs:\n",
    "#             folder_names.append(os.path.join(root, dir_name))  # Full path to the folder\n",
    "#     return folder_names\n",
    "\n",
    "# # Example usage\n",
    "# base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "# folders = get_folder_names(base_path)\n",
    "# print(folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375596ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.406985Z",
     "iopub.status.busy": "2025-01-22T17:09:23.406800Z",
     "iopub.status.idle": "2025-01-22T17:09:23.409318Z",
     "shell.execute_reply": "2025-01-22T17:09:23.408778Z"
    },
    "papermill": {
     "duration": 0.007209,
     "end_time": "2025-01-22T17:09:23.410522",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.403313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Full paths list\n",
    "# folder_paths = ['/kaggle/input/newari-food-dataset/newa_food_dataset/sapumicha', '/kaggle/input/newari-food-dataset/newa_food_dataset/chhoila', '/kaggle/input/newari-food-dataset/newa_food_dataset/Voj set full', '/kaggle/input/newari-food-dataset/newa_food_dataset/Thoo', '/kaggle/input/newari-food-dataset/newa_food_dataset/yomari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Goramari', '/kaggle/input/newari-food-dataset/newa_food_dataset/kauli chana', '/kaggle/input/newari-food-dataset/newa_food_dataset/Saag', '/kaggle/input/newari-food-dataset/newa_food_dataset/chicken', '/kaggle/input/newari-food-dataset/newa_food_dataset/Bhuti', '/kaggle/input/newari-food-dataset/newa_food_dataset/Paun Kwa', '/kaggle/input/newari-food-dataset/newa_food_dataset/Dhau', '/kaggle/input/newari-food-dataset/newa_food_dataset/Channa', '/kaggle/input/newari-food-dataset/newa_food_dataset/Phokso Fry', '/kaggle/input/newari-food-dataset/newa_food_dataset/Lain tarkari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Fish', '/kaggle/input/newari-food-dataset/newa_food_dataset/labha palu musya', '/kaggle/input/newari-food-dataset/newa_food_dataset/Chatamari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Aila', '/kaggle/input/newari-food-dataset/newa_food_dataset/golveda achar', '/kaggle/input/newari-food-dataset/newa_food_dataset/Bhutan', '/kaggle/input/newari-food-dataset/newa_food_dataset/Chhoila Baji', '/kaggle/input/newari-food-dataset/newa_food_dataset/Mutumari', '/kaggle/input/newari-food-dataset/newa_food_dataset/Kachilaa', '/kaggle/input/newari-food-dataset/newa_food_dataset/Bara', '/kaggle/input/newari-food-dataset/newa_food_dataset/Baji', '/kaggle/input/newari-food-dataset/newa_food_dataset/yomari/yomari']\n",
    "# # Extract only folder names\n",
    "# folder_names = [os.path.basename(path) for path in folder_paths]\n",
    "\n",
    "# # Print the result\n",
    "# print(folder_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77e3d41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.417306Z",
     "iopub.status.busy": "2025-01-22T17:09:23.417108Z",
     "iopub.status.idle": "2025-01-22T17:09:23.419766Z",
     "shell.execute_reply": "2025-01-22T17:09:23.419016Z"
    },
    "papermill": {
     "duration": 0.007372,
     "end_time": "2025-01-22T17:09:23.420946",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.413574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install google-images-download\n",
    "# !pip install pillow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bb058",
   "metadata": {
    "papermill": {
     "duration": 0.002843,
     "end_time": "2025-01-22T17:09:23.426790",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.423947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Python Code for Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8822d1a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.433861Z",
     "iopub.status.busy": "2025-01-22T17:09:23.433674Z",
     "iopub.status.idle": "2025-01-22T17:09:23.436919Z",
     "shell.execute_reply": "2025-01-22T17:09:23.436340Z"
    },
    "papermill": {
     "duration": 0.007911,
     "end_time": "2025-01-22T17:09:23.438037",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.430126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import imgaug.augmenters as iaa\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def augment_images_in_folder(input_folder, output_folder, target_count=600):\n",
    "#     \"\"\"\n",
    "#     Augments images in the input folder to ensure at least `target_count` images exist in the output folder.\n",
    "#     \"\"\"\n",
    "#     # Create the output folder if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "#     # List of all image files in the input folder\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "#     image_count = len(image_files)\n",
    "    \n",
    "#     # If the folder already has enough images, skip augmentation\n",
    "#     if image_count >= target_count:\n",
    "#         print(f\"{input_folder} already has {image_count} images, skipping augmentation.\")\n",
    "#         return\n",
    "\n",
    "#     # Load all images\n",
    "#     images = [cv2.imread(os.path.join(input_folder, f)) for f in image_files]\n",
    "#     images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images if img is not None]\n",
    "\n",
    "#     # Define augmentation pipeline\n",
    "#     augmenter = iaa.Sequential([\n",
    "#         iaa.Fliplr(0.5),  # Horizontal flip\n",
    "#         iaa.Affine(scale=(0.8, 1.2), rotate=(-25, 25)),  # Scaling and rotation\n",
    "#         iaa.AdditiveGaussianNoise(scale=(10, 20)),  # Adding noise\n",
    "#         iaa.Multiply((0.8, 1.2)),  # Brightness adjustment\n",
    "#         iaa.Crop(percent=(0, 0.1))  # Random cropping\n",
    "#     ])\n",
    "\n",
    "#     # Augment images until we reach the target count\n",
    "#     augmented_count = 0\n",
    "#     for _ in tqdm(range(target_count - image_count), desc=f\"Augmenting {input_folder}\"):\n",
    "#         # Choose a random image to augment\n",
    "#         random_image = images[np.random.randint(0, len(images))]\n",
    "#         augmented_image = augmenter(image=random_image)\n",
    "\n",
    "#         # Save the augmented image\n",
    "#         output_path = os.path.join(output_folder, f\"aug_{augmented_count + image_count}.jpg\")\n",
    "#         cv2.imwrite(output_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "#         augmented_count += 1\n",
    "\n",
    "#     print(f\"Augmented {augmented_count} images in {input_folder}.\")\n",
    "\n",
    "\n",
    "# def augment_dataset(base_path, output_base_path, target_count=250):\n",
    "#     \"\"\"\n",
    "#     Iterates through each folder in the base path and augments images to reach the target count.\n",
    "#     \"\"\"\n",
    "#     folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "#     for folder in folders:\n",
    "#         input_folder = os.path.join(base_path, folder)\n",
    "#         output_folder = os.path.join(output_base_path, folder)\n",
    "#         augment_images_in_folder(input_folder, output_folder, target_count)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "# output_base_path = \"/kaggle/working/augmented_dataset\"  # Output directory for augmented images\n",
    "# target_count = 600  # Target number of images per folder\n",
    "\n",
    "# augment_dataset(base_path, output_base_path, target_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaea4417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.445051Z",
     "iopub.status.busy": "2025-01-22T17:09:23.444852Z",
     "iopub.status.idle": "2025-01-22T17:09:23.447987Z",
     "shell.execute_reply": "2025-01-22T17:09:23.447391Z"
    },
    "papermill": {
     "duration": 0.007991,
     "end_time": "2025-01-22T17:09:23.449200",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.441209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import imgaug.augmenters as iaa\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def augment_images_in_folder(input_folder, output_folder, target_count=600):\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "#     image_count = len(image_files)\n",
    "#     if image_count >= target_count:\n",
    "#         print(f\"{input_folder} already has {image_count} images, skipping augmentation.\")\n",
    "#         return\n",
    "\n",
    "#     images = [cv2.imread(os.path.join(input_folder, f)) for f in image_files]\n",
    "#     images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images if img is not None]\n",
    "\n",
    "#     augmenter = iaa.Sequential([\n",
    "#         iaa.Fliplr(0.5),\n",
    "#         iaa.Affine(scale=(0.8, 1.2), rotate=(-25, 25)),\n",
    "#         iaa.AdditiveGaussianNoise(scale=(10, 20)),\n",
    "#         iaa.Multiply((0.8, 1.2)),\n",
    "#         iaa.Crop(percent=(0, 0.1))\n",
    "#     ])\n",
    "\n",
    "#     augmented_count = 0\n",
    "#     for _ in tqdm(range(target_count - image_count), desc=f\"Augmenting {input_folder}\"):\n",
    "#         random_image = images[np.random.randint(0, len(images))]\n",
    "#         augmented_image = augmenter(image=random_image)\n",
    "#         output_path = os.path.join(output_folder, f\"aug_{augmented_count + image_count}.jpg\")\n",
    "#         cv2.imwrite(output_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "#         augmented_count += 1\n",
    "\n",
    "#     print(f\"Augmented {augmented_count} images in {input_folder}.\")\n",
    "\n",
    "# def augment_dataset(base_path, output_base_path, target_count=600, folder_indices=None):\n",
    "#     if folder_indices is None:\n",
    "#         folder_indices = []\n",
    "#     folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "#     selected_folders = [folders[i] for i in folder_indices]\n",
    "#     for folder in selected_folders:\n",
    "#         input_folder = os.path.join(base_path, folder)\n",
    "#         output_folder = os.path.join(output_base_path, folder)\n",
    "#         augment_images_in_folder(input_folder, output_folder, target_count)\n",
    "\n",
    "# # Base paths\n",
    "# base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "# output_base_path = \"/kaggle/working/augmented_dataset\"\n",
    "# target_count = 600\n",
    "\n",
    "# # First 8 folders (indices 0 to 7)\n",
    "# augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85eae83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:09:23.456104Z",
     "iopub.status.busy": "2025-01-22T17:09:23.455911Z",
     "iopub.status.idle": "2025-01-22T17:17:57.497618Z",
     "shell.execute_reply": "2025-01-22T17:17:57.496649Z"
    },
    "papermill": {
     "duration": 514.046862,
     "end_time": "2025-01-22T17:17:57.499170",
     "exception": false,
     "start_time": "2025-01-22T17:09:23.452308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting /kaggle/input/newa-food-data/final_dataset/Lainachar: 100%|██████████| 433/433 [02:52<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented 433 images in /kaggle/input/newa-food-data/final_dataset/Lainachar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting /kaggle/input/newa-food-data/final_dataset/Bara: 100%|██████████| 475/475 [01:27<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented 475 images in /kaggle/input/newa-food-data/final_dataset/Bara.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting /kaggle/input/newa-food-data/final_dataset/egg: 100%|██████████| 469/469 [02:24<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented 469 images in /kaggle/input/newa-food-data/final_dataset/egg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting /kaggle/input/newa-food-data/final_dataset/Baji: 100%|██████████| 473/473 [01:01<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented 473 images in /kaggle/input/newa-food-data/final_dataset/Baji.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_images_in_folder(input_folder, output_folder, target_count=600):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    image_count = len(image_files)\n",
    "    if image_count >= target_count:\n",
    "        print(f\"{input_folder} already has {image_count} images, skipping augmentation.\")\n",
    "        return\n",
    "\n",
    "    images = [cv2.imread(os.path.join(input_folder, f)) for f in image_files]\n",
    "    images = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in images if img is not None]\n",
    "\n",
    "    augmenter = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Affine(scale=(0.8, 1.2), rotate=(-25, 25)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(10, 20)),\n",
    "        iaa.Multiply((0.8, 1.2)),\n",
    "        iaa.Crop(percent=(0, 0.1))\n",
    "    ])\n",
    "\n",
    "    augmented_count = 0\n",
    "    for _ in tqdm(range(target_count - image_count), desc=f\"Augmenting {input_folder}\"):\n",
    "        random_image = images[np.random.randint(0, len(images))]\n",
    "        augmented_image = augmenter(image=random_image)\n",
    "        output_path = os.path.join(output_folder, f\"aug_{augmented_count + image_count}.jpg\")\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "        augmented_count += 1\n",
    "\n",
    "    print(f\"Augmented {augmented_count} images in {input_folder}.\")\n",
    "\n",
    "def augment_dataset(base_path, output_base_path, target_count=600, folder_indices=None):\n",
    "    if folder_indices is None:\n",
    "        folder_indices = []\n",
    "    folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "    selected_folders = [folders[i] for i in folder_indices]\n",
    "    for folder in selected_folders:\n",
    "        input_folder = os.path.join(base_path, folder)\n",
    "        output_folder = os.path.join(output_base_path, folder)\n",
    "        augment_images_in_folder(input_folder, output_folder, target_count)\n",
    "\n",
    "# Base paths\n",
    "base_path = \"/kaggle/input/newa-food-data/final_dataset\"\n",
    "output_base_path = \"/kaggle/working/augmented_dataset_part3\"\n",
    "target_count = 600\n",
    "\n",
    "# First 8 folders (indices 0 to 7)\n",
    "# augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8)))\n",
    "\n",
    "# Second batch: Remaining 8 folders (indices 8 to 15)\n",
    "augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(12, 16)))\n",
    "\n",
    "# augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8, 12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f40328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:17:57.592346Z",
     "iopub.status.busy": "2025-01-22T17:17:57.591978Z",
     "iopub.status.idle": "2025-01-22T17:17:57.594949Z",
     "shell.execute_reply": "2025-01-22T17:17:57.594320Z"
    },
    "papermill": {
     "duration": 0.050086,
     "end_time": "2025-01-22T17:17:57.596068",
     "exception": false,
     "start_time": "2025-01-22T17:17:57.545982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Second batch: Remaining 8 folders (indices 8 to 15)\n",
    "# augment_dataset(base_path, output_base_path, target_count, folder_indices=list(range(8, 16)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e661b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:17:57.694667Z",
     "iopub.status.busy": "2025-01-22T17:17:57.694361Z",
     "iopub.status.idle": "2025-01-22T17:21:14.242020Z",
     "shell.execute_reply": "2025-01-22T17:21:14.241173Z"
    },
    "papermill": {
     "duration": 196.683318,
     "end_time": "2025-01-22T17:21:14.331369",
     "exception": false,
     "start_time": "2025-01-22T17:17:57.648051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped folder created at: /kaggle/working/augmented_dataset_600_part3.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the folder to zip and the output zip file path\n",
    "folder_to_zip = \"/kaggle/working/augmented_dataset_part3\"\n",
    "output_zip_file = \"/kaggle/working/augmented_dataset_600_part3.zip\"\n",
    "\n",
    "# Create a zip archive of the folder2\n",
    "shutil.make_archive(base_name=output_zip_file.replace('.zip', ''), format='zip', root_dir=folder_to_zip)\n",
    "\n",
    "print(f\"Zipped folder created at: {output_zip_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c940c",
   "metadata": {
    "papermill": {
     "duration": 0.046154,
     "end_time": "2025-01-22T17:21:14.423169",
     "exception": false,
     "start_time": "2025-01-22T17:21:14.377015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15219f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T17:21:14.515090Z",
     "iopub.status.busy": "2025-01-22T17:21:14.514814Z",
     "iopub.status.idle": "2025-01-22T17:21:14.518563Z",
     "shell.execute_reply": "2025-01-22T17:21:14.517884Z"
    },
    "papermill": {
     "duration": 0.051427,
     "end_time": "2025-01-22T17:21:14.519766",
     "exception": false,
     "start_time": "2025-01-22T17:21:14.468339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "# import zipfile\n",
    "\n",
    "# # Define paths\n",
    "# input_base_path = '/kaggle/input/newari-food-augumented-dataset'\n",
    "# working_base_path = '/kaggle/working/newari-food-augmented-dataset'\n",
    "\n",
    "# # Copy dataset to a writable directory\n",
    "# def copy_dataset(input_path, working_path):\n",
    "#     if not os.path.exists(working_path):\n",
    "#         shutil.copytree(input_path, working_path)\n",
    "#     print(f\"Dataset copied to {working_path}\")\n",
    "\n",
    "# # Define proportions\n",
    "# train_ratio = 0.8\n",
    "# test_ratio = 0.1\n",
    "# val_ratio = 0.1\n",
    "\n",
    "# # Ensure the ratios add up to 1\n",
    "# assert train_ratio + test_ratio + val_ratio == 1, \"Ratios must sum to 1.\"\n",
    "\n",
    "# # Function to split dataset\n",
    "# def split_dataset(base_path):\n",
    "#     for class_name in os.listdir(base_path):\n",
    "#         class_path = os.path.join(base_path, class_name)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "\n",
    "#         # Get all images in the current class folder\n",
    "#         images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "\n",
    "#         # Shuffle the images\n",
    "#         random.shuffle(images)\n",
    "\n",
    "#         # Compute split indices\n",
    "#         total_images = len(images)\n",
    "#         train_count = int(total_images * train_ratio)\n",
    "#         test_count = int(total_images * test_ratio)\n",
    "\n",
    "#         # Split images into train, test, and validation sets\n",
    "#         train_images = images[:train_count]\n",
    "#         test_images = images[train_count:train_count + test_count]\n",
    "#         val_images = images[train_count + test_count:]\n",
    "\n",
    "#         # Create train, test, and validation directories\n",
    "#         train_dir = os.path.join(class_path, 'train')\n",
    "#         test_dir = os.path.join(class_path, 'test')\n",
    "#         val_dir = os.path.join(class_path, 'val')\n",
    "\n",
    "#         os.makedirs(train_dir, exist_ok=True)\n",
    "#         os.makedirs(test_dir, exist_ok=True)\n",
    "#         os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "#         # Move images to respective folders\n",
    "#         for img in train_images:\n",
    "#             shutil.move(os.path.join(class_path, img), os.path.join(train_dir, img))\n",
    "\n",
    "#         for img in test_images:\n",
    "#             shutil.move(os.path.join(class_path, img), os.path.join(test_dir, img))\n",
    "\n",
    "#         for img in val_images:\n",
    "#             shutil.move(os.path.join(class_path, img), os.path.join(val_dir, img))\n",
    "\n",
    "#         print(f\"Class '{class_name}': {len(train_images)} train, {len(test_images)} test, {len(val_images)} val\")\n",
    "\n",
    "# # Function to zip the dataset\n",
    "# def zip_dataset(base_path, output_zip):\n",
    "#     with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#         for root, dirs, files in os.walk(base_path):\n",
    "#             for file in files:\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 arcname = os.path.relpath(file_path, base_path)\n",
    "#                 zipf.write(file_path, arcname)\n",
    "#     print(f\"Zipped dataset saved to {output_zip}\")\n",
    "\n",
    "# # Copy the dataset to a writable directory\n",
    "# copy_dataset(input_base_path, working_base_path)\n",
    "\n",
    "# # Run the split function\n",
    "# split_dataset(working_base_path)\n",
    "\n",
    "# # Create a zip file of the split dataset\n",
    "# output_zip = '/kaggle/working/newari_food_split_dataset.zip'\n",
    "# zip_dataset(working_base_path, output_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f6bc59",
   "metadata": {
    "papermill": {
     "duration": 0.046597,
     "end_time": "2025-01-22T17:21:14.612245",
     "exception": false,
     "start_time": "2025-01-22T17:21:14.565648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'aloo': 354 train, 89 test\n",
      "Class 'Baji': 378 train, 95 test\n",
      "Class 'Bara': 380 train, 95 test\n",
      "Class 'Bhutan': 364 train, 92 test\n",
      "Class 'Bhuti': 360 train, 90 test\n",
      "Class 'Buffalo Masu(Dakulaa)': 384 train, 97 test\n",
      "Class 'Channa': 366 train, 92 test\n",
      "Class 'Dhau': 359 train, 90 test\n",
      "Class 'egg': 375 train, 94 test\n",
      "Class 'Kachilaa': 409 train, 103 test\n",
      "Class 'Lain tarkari': 450 train, 113 test\n",
      "Class 'Lainachar': 346 train, 87 test\n",
      "Class 'Saag': 372 train, 94 test\n",
      "Class 'sapumicha': 362 train, 91 test\n",
      "Class 'test': 0 train, 0 test\n",
      "Class 'Thoo': 397 train, 100 test\n",
      "Class 'train': 0 train, 0 test\n",
      "Class 'yomari': 373 train, 94 test\n",
      "Zipped dataset saved to final_splitted_dataset.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def split_dataset(input_base_path, output_base_path, train_ratio=0.8):\n",
    "    train_dir = os.path.join(output_base_path, 'train')\n",
    "    test_dir = os.path.join(output_base_path, 'test')\n",
    "    \n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    for class_name in os.listdir(input_base_path):\n",
    "        class_path = os.path.join(input_base_path, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        total_images = len(images)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        \n",
    "        train_images = images[:train_count]\n",
    "        test_images = images[train_count:]\n",
    "        \n",
    "        class_train_dir = os.path.join(train_dir, class_name)\n",
    "        class_test_dir = os.path.join(test_dir, class_name)\n",
    "        \n",
    "        os.makedirs(class_train_dir, exist_ok=True)\n",
    "        os.makedirs(class_test_dir, exist_ok=True)\n",
    "        \n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(class_train_dir, img))\n",
    "        \n",
    "        for img in test_images:\n",
    "            shutil.copy(os.path.join(class_path, img), os.path.join(class_test_dir, img))\n",
    "        \n",
    "        print(f\"Class '{class_name}': {len(train_images)} train, {len(test_images)} test\")\n",
    "\n",
    "def zip_dataset(base_path, output_zip):\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, base_path)\n",
    "                zipf.write(file_path, arcname)\n",
    "    print(f\"Zipped dataset saved to {output_zip}\")\n",
    "\n",
    "# Define paths\n",
    "input_base_path = 'augmented_dataset_600'  # Original dataset\n",
    "output_base_path = 'final_splitted_dataset'  # New dataset location\n",
    "output_zip = 'final_splitted_dataset.zip'  # Output zip file\n",
    "\n",
    "# Run the split function\n",
    "split_dataset(input_base_path, output_base_path)\n",
    "\n",
    "# Zip the final dataset\n",
    "zip_dataset(output_base_path, output_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00ed89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6512522,
     "sourceId": 10522682,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6517542,
     "sourceId": 10532705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6519962,
     "sourceId": 10537376,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 714.324243,
   "end_time": "2025-01-22T17:21:15.178184",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-22T17:09:20.853941",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
